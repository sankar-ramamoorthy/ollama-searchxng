Below is a **clean, production-ready README.md** for your multi-service AI application.
It assumes your repo contains:

* `frontend_svc/`
* `backend_svc/`
* `searchxng_svc/`
* `docker-compose.yml`
* Your custom tools, prompts, and LLM pipeline.

You can copy/paste this directly as **README.md** at the project root.

---

# ğŸ§  AI Chatbot with Tool Calling â€” Full Stack (Frontend + Backend + SearchXNG)

This project implements a **full AI agent system** using:

* **Ollama** for LLM inference
* **Custom tool calling** (weather, date, and web search)
* **SearchXNG** as the search backend
* **FastAPI** backend for orchestrating requests
* **Gradio** frontend chat UI
* **Docker Compose** for running the entire system

The LLM automatically decides whether to call one of the available tools:

* `searchxng`
* `get_weather`
* `get_date`

It then processes the tool result and returns a final answer to the user.

---

## ğŸš€ Features

### âœ”ï¸ Intelligent tool-calling

The backend injects tool routing instructions into the LLMâ€™s prompt.
The model can call at most **one tool per turn**, and tool results are fed back into a second LLM pass to generate a clean final response.

### âœ”ï¸ Web search using SearchXNG

Search queries like:

```
Who is the prime minister of Japan?
What is the population of Brazil?
```

trigger the `searchxng` tool automatically.

### âœ”ï¸ Weather querying

Query example:

```
What is the weather in New York?
```

Triggers the internal weather lookup tool.

### âœ”ï¸ Robust date tool

Explicitly called only when the user clearly asks for the date:

```
What is today's date?
What day is it?
```

### âœ”ï¸ Gradio Frontend

A simple, clean web UI for interacting with the agent.

### âœ”ï¸ Fully containerized

All services run through docker-compose:

* **frontend_svc** (Gradio)
* **backend_svc** (FastAPI + tool routing + LLM integration)
* **searchxng_svc** (Search)
* **Ollama** (LLM runtime)

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ README.md
â”œâ”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ backend_svc/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ prompts.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ get_weather.py
â”‚   â”‚   â”œâ”€â”€ get_date.py
â”‚   â”‚   â”œâ”€â”€ searchxng.py
â”‚   â”‚   â”œâ”€â”€ tool_schemas.py
â”‚   â”‚   â”œâ”€â”€ *.json
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ frontend_svc/
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â””â”€â”€ searchxng_svc/
    â””â”€â”€ (SearchXNG server)
```

---

## ğŸ› ï¸ Requirements

* Docker
* Docker Compose
* At least one local model installed in Ollama, e.g.:

```
ollama pull granite4:350m
```

You can also configure other models in `backend_svc/app.py`.

---

## â–¶ï¸ Running The Project

From the project root:

```bash
docker-compose up --build
```

This launches:

* **Ollama** on port `11434`
* **Backend** on port `8000`
* **Frontend** on port `7860`
* **SearchXNG** on port `8080`

Once running:

### Open the Chat UI:

ğŸ‘‰ [http://localhost:7860](http://localhost:7860)

### Backend docs:

ğŸ‘‰ [http://localhost:8000/docs](http://localhost:8000/docs)

---

## ğŸ’¬ Example Queries

| Query                                 | Expected Tool | Result               |
| ------------------------------------- | ------------- | -------------------- |
| â€œWho is the prime minister of Japan?â€ | searchxng     | Uses recent news     |
| â€œWeather in Toronto?â€                 | get_weather   | Weather summary      |
| â€œWhat is todayâ€™s date?â€               | get_date      | Returns current date |
| â€œExplain black holesâ€                 | none          | Direct LLM answer    |

---

## âš™ï¸ How Tool Calling Works

### 1ï¸âƒ£ First LLM Pass â†’ Decide tool or answer directly

The LLM sees rules for tool routing and returns either:

* A normal text response
* OR a `tool_call` block like:

```json
{
  "tool_calls": [
    {
      "id": "call_123",
      "function": {
        "name": "searchxng",
        "arguments": { "query": "prime minister of Japan" }
      }
    }
  ]
}
```

### 2ï¸âƒ£ Backend runs the tool

The backend parses the tool call, routes to:

* `tools/searchxng.py`
* `tools/get_weather.py`
* `tools/get_date.py`

and captures the output.

### 3ï¸âƒ£ Second LLM Pass â†’ Final Answer

The backend creates a **follow-up prompt** with:

* The user question
* The tool result
* Strict instructions to answer directly

This eliminates hallucinations.

---

## ğŸ§© Troubleshooting

### ğŸ”¸ The model picks the wrong tool

This is expected with small models (350M).
Tool routing is controlled in `prompts.py`:

* Tighten rules
* Add forbidden patterns
* Upgrade to a larger model

### ğŸ”¸ SearchXNG returns too many results

Adjust `count=` inside `tools/searchxng.py`.

### ğŸ”¸ Empty or messy LLM outputs

Inspect backend logs:

```
docker logs backend_svc
```

---

## ğŸ§ª Running Backend Without Docker

Inside `backend_svc/`:

```bash
uvicorn app:app --reload --host 0.0.0.0 --port 8000
```

---

## ğŸ§± Customizing Tools

Add a new tool in:

```
backend_svc/tools/
```

Add its JSON schema in:

```
backend_svc/tools/*.json
```

Then expose it in:

```
backend_svc/tools/__init__.py
```

The backend automatically includes it in the tool-calling prompt.

---



---

## ğŸ“ License

MIT License

---
